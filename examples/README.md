# Please refer Intel速 Gaudi速 README for Gaudi examples

> [!NOTE]
> Not all examples in this folder are Intel Gaudi specific and come from the original vllm-project repository from where this fork was created. For examples such as offline inference with openAI on Intel Gaudi please refer to [Intel速 Gaudi速 README supported features table](https://github.com/HabanaAI/vllm-fork/blob/v0.7.2%2BGaudi-1.21.0/README_GAUDI.md#supported-features) and the [quantization section](https://github.com/HabanaAI/vllm-fork/blob/v0.7.2%2BGaudi-1.21.0/README_GAUDI.md#quantization-fp8-inference-and-model-calibration-process) for FP8 examples.
